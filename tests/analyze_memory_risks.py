# analyze_memory_risks.py
"""
Anal√Ωza zn√°m√Ωch rizik v Cognitive Memory syst√©mu.

Detekuje a kvantifikuje:
1. Catastrophic interference (prol√≠n√°n√≠ nesouvisej√≠c√≠ch vzpom√≠nek)
2. Capacity saturation (vyƒçerp√°n√≠ kapacity)
3. Diffusion instability (nestabilita difuze)
4. Consolidation failure (selh√°n√≠ STM‚ÜíLTM)
5. Memory leak (ne≈æ√°douc√≠ r≈Øst pamƒõti)
6. Temporal bias (zapomnƒõn√≠ star√Ωch vzpom√≠nek)
"""

import sys
from pathlib import Path

# P≈ôidej parent directory do sys.path
sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
from pathlib import Path
import json
from typing import Dict, List, Tuple
from dataclasses import dataclass, asdict
import warnings

from cognitive_memory import load_memory_state


@dataclass
class RiskAssessment:
    """Struktura pro hodnocen√≠ rizika."""
    risk_name: str
    severity: str  # "LOW", "MEDIUM", "HIGH", "CRITICAL"
    score: float  # 0-1
    description: str
    details: Dict
    recommendations: List[str]


class MemoryRiskAnalyzer:
    """Analyz√©r rizik pro Cognitive Memory."""
    
    def __init__(self, results_dir: str = "stress_test_results"):
        self.results_dir = Path(results_dir)
        self.metrics_df = None
        self.consolidation_events = None
        self.risks: List[RiskAssessment] = []
        
        # Naƒçti data
        self._load_data()
    
    def _load_data(self):
        """Naƒçte metriky a eventy."""
        # Hledej metrics_*.csv (podporuje r≈Øzn√© n√°zvy sc√©n√°≈ô≈Ø)
        metrics_files = list(self.results_dir.glob("metrics_*.csv"))
        
        if metrics_files:
            # Naƒçti prvn√≠ nalezen√Ω soubor
            metrics_path = metrics_files[0]
            self.metrics_df = pd.read_csv(metrics_path)
            print(f"‚úì Loaded metrics: {metrics_path.name}")
        else:
            print(f"‚ö† No metrics found in {self.results_dir}")
        
        consol_path = self.results_dir / "consolidation_events.json"
        if consol_path.exists():
            with open(consol_path, 'r') as f:
                self.consolidation_events = json.load(f)
            print(f"‚úì Loaded {len(self.consolidation_events)} consolidation events")
        else:
            print(f"‚ö† No consolidation events found")
    
    def analyze_catastrophic_interference(self) -> RiskAssessment:
        """
        Riziko 1: Catastrophic Interference
        
        Detekce: Sleduje, zda nov√© vzpom√≠nky p≈ôepisuj√≠ star√©.
        Metriky:
        - Pokles retention rate
        - P≈ôekr√Ωv√°n√≠ cluster≈Ø (pokud jsou k dispozici)
        - Fluktuace v intenzitƒõ center
        """
        if self.metrics_df is None:
            return self._no_data_risk("Catastrophic Interference")
        
        df = self.metrics_df
        
        # Proxy: pokud poƒçet aktivn√≠ch center kles√°, m≈Ø≈æe doch√°zet k p≈ôepisov√°n√≠
        n_active = df['n_active'].values
        
        # Pokles v posledn√≠ch 20% dat
        split_point = int(len(n_active) * 0.8)
        early_mean = n_active[:split_point].mean()
        late_mean = n_active[split_point:].mean()
        
        retention_loss = max(0, 1 - (late_mean / early_mean)) if early_mean > 0 else 0
        
        # Fluktuace write strength (vysok√° variance = nep≈ôedv√≠dateln√© p≈ôepisov√°n√≠)
        omega_variance = df['omega_mean'].var()
        
        # Kombinovan√Ω score
        interference_score = 0.5 * retention_loss + 0.5 * min(1.0, omega_variance / 0.01)
        
        # Severity
        if interference_score < 0.2:
            severity = "LOW"
            color = "üü¢"
        elif interference_score < 0.5:
            severity = "MEDIUM"
            color = "üü°"
        elif interference_score < 0.8:
            severity = "HIGH"
            color = "üü†"
        else:
            severity = "CRITICAL"
            color = "üî¥"
        
        recommendations = []
        if interference_score > 0.3:
            recommendations.append("Zvƒõt≈°it kapacitu LTM (n_ltm_centers)")
            recommendations.append("Sn√≠≈æit write_strength_base (pomalej≈°√≠ z√°pis)")
            recommendations.append("Zv√Ω≈°it merge_similarity_threshold (m√©nƒõ mergov√°n√≠)")
        
        return RiskAssessment(
            risk_name=f"{color} Catastrophic Interference",
            severity=severity,
            score=interference_score,
            description="Riziko p≈ôepisov√°n√≠ star√Ωch vzpom√≠nek nov√Ωmi",
            details={
                "retention_loss": f"{retention_loss:.2%}",
                "omega_variance": f"{omega_variance:.4f}",
                "early_active_mean": int(early_mean),
                "late_active_mean": int(late_mean)
            },
            recommendations=recommendations
        )
    
    def analyze_capacity_saturation(self) -> RiskAssessment:
        """
        Riziko 2: Capacity Saturation
        
        Detekce: Syst√©m vyƒçerp√°v√° dostupnou kapacitu.
        Metriky:
        - Pod√≠l aktivn√≠ch center k maximu
        - Frekvence mergov√°n√≠
        - Frekvence prune operac√≠
        """
        if self.metrics_df is None:
            return self._no_data_risk("Capacity Saturation")
        
        df = self.metrics_df
        
        # Fin√°ln√≠ vyu≈æit√≠ kapacity
        final_active = df['n_active'].iloc[-1]
        max_capacity = df['n_total'].iloc[-1]
        capacity_usage = final_active / max_capacity
        
        # Trend r≈Østu (line√°rn√≠ fit)
        steps = df['step'].values
        active = df['n_active'].values
        
        if len(steps) > 100:
            # Posledn√≠ 20%
            tail_steps = steps[-len(steps)//5:]
            tail_active = active[-len(active)//5:]
            
            # Slope
            slope = np.polyfit(tail_steps, tail_active, 1)[0]
            
            # Extrapolace kdy se napln√≠
            if slope > 0:
                steps_to_full = (max_capacity - final_active) / slope
            else:
                steps_to_full = float('inf')
        else:
            steps_to_full = float('inf')
        
        # Score
        saturation_score = capacity_usage
        if steps_to_full < 5000:  # M√©nƒõ ne≈æ 5000 krok≈Ø do naplnƒõn√≠
            saturation_score = max(saturation_score, 0.7)
        
        # Severity
        if saturation_score < 0.5:
            severity = "LOW"
            color = "üü¢"
        elif saturation_score < 0.75:
            severity = "MEDIUM"
            color = "üü°"
        elif saturation_score < 0.9:
            severity = "HIGH"
            color = "üü†"
        else:
            severity = "CRITICAL"
            color = "üî¥"
        
        recommendations = []
        if saturation_score > 0.6:
            recommendations.append("Zv√Ω≈°it max_centers_ltm (vƒõt≈°√≠ kapacita)")
            recommendations.append("Sn√≠≈æit new_center_threshold (m√©nƒõ nov√Ωch center)")
            recommendations.append("Zv√Ω≈°it prune_intensity_threshold (agresivnƒõj≈°√≠ prune)")
        
        return RiskAssessment(
            risk_name=f"{color} Capacity Saturation",
            severity=severity,
            score=saturation_score,
            description="Riziko vyƒçerp√°n√≠ dostupn√© kapacity pamƒõti",
            details={
                "capacity_usage": f"{capacity_usage:.2%}",
                "active_centers": int(final_active),
                "max_capacity": int(max_capacity),
                "steps_to_full": int(steps_to_full) if steps_to_full != float('inf') else "‚àû",
                "growth_slope": f"{slope:.2f}" if 'slope' in locals() else "N/A"
            },
            recommendations=recommendations
        )
    
    def analyze_diffusion_stability(self) -> RiskAssessment:
        """
        Riziko 3: Diffusion Instability
        
        Detekce: Nestabiln√≠ difuze m≈Ø≈æe v√©st k explozi hodnot.
        Metriky:
        - H_max > threshold (exploding values)
        - E_max > 2.0 (emoce mimo rozsah)
        - Fluktuace v H_mean
        """
        if self.metrics_df is None:
            return self._no_data_risk("Diffusion Instability")
        
        df = self.metrics_df
        
        # Maxim√°ln√≠ hodnoty
        h_max_peak = df['H_max'].max()
        h_mean_std = df['H_mean'].std()
        
        # E_max (pokud je v datech)
        if 'E_max' in df.columns:
            e_max_peak = df['E_max'].max()
        else:
            e_max_peak = 0
        
        # CFL stability condition: Œ± ‚â§ 1/6
        # Pokud Œ±_H = 0.002, je stabiln√≠
        # Ale m≈Ø≈æeme kontrolovat r≈Øst hodnot
        
        # Score based on values
        instability_score = 0
        
        # 1. H_max by nemƒõl r≈Øst do nekoneƒçna
        if h_max_peak > 5.0:  # Arbitrary threshold
            instability_score += 0.4
        
        # 2. Velk√° variance v H_mean = nestabilita
        if h_mean_std > 0.5:
            instability_score += 0.3
        
        # 3. Emoce mimo rozsah [0.5, 1.5]
        if e_max_peak > 2.0:
            instability_score += 0.3
        
        instability_score = min(1.0, instability_score)
        
        # Severity
        if instability_score < 0.2:
            severity = "LOW"
            color = "üü¢"
        elif instability_score < 0.5:
            severity = "MEDIUM"
            color = "üü°"
        elif instability_score < 0.8:
            severity = "HIGH"
            color = "üü†"
        else:
            severity = "CRITICAL"
            color = "üî¥"
        
        recommendations = []
        if instability_score > 0.3:
            recommendations.append("Zkontrolovat CFL podm√≠nku: Œ±_H ‚â§ 1/6")
            recommendations.append("Sn√≠≈æit terrain_ltm_alpha_h (pomalej≈°√≠ difuze)")
            recommendations.append("Zv√Ω≈°it terrain_ltm_lambda (silnƒõj≈°√≠ decay)")
        
        return RiskAssessment(
            risk_name=f"{color} Diffusion Instability",
            severity=severity,
            score=instability_score,
            description="Riziko nestabiln√≠ difuze v 3D ter√©nu",
            details={
                "H_max_peak": f"{h_max_peak:.3f}",
                "H_mean_std": f"{h_mean_std:.3f}",
                "E_max_peak": f"{e_max_peak:.3f}" if e_max_peak > 0 else "N/A",
                "CFL_condition": "Œ±_H ‚â§ 0.1667 (should be 0.002)"
            },
            recommendations=recommendations
        )
    
    def analyze_consolidation_failure(self) -> RiskAssessment:
        """
        Riziko 4: Consolidation Failure
        
        Detekce: STM‚ÜíLTM konsolidace nefunguje spr√°vnƒõ.
        Metriky:
        - Frekvence konsolidac√≠ (p≈ô√≠li≈° ƒçasto/m√°lo)
        - Poƒçet konsolidovan√Ωch center (p≈ô√≠li≈° m√°lo)
        - R≈Øst √∫navy bez konsolidace
        """
        if not self.consolidation_events:
            return RiskAssessment(
                risk_name="‚ö™ Consolidation Failure",
                severity="UNKNOWN",
                score=0.5,
                description="≈Ω√°dn√© konsolidaƒçn√≠ eventy nenalezeny",
                details={},
                recommendations=["Spustit del≈°√≠ simulaci pro z√≠sk√°n√≠ dat"]
            )
        
        df_consol = pd.DataFrame(self.consolidation_events)
        total_steps = self.metrics_df['step'].max()
        
        # Frekvence konsolidac√≠
        n_consolidations = len(df_consol)
        consolidation_frequency = total_steps / n_consolidations if n_consolidations > 0 else float('inf')
        
        # Pr≈Ømƒõrn√Ω poƒçet konsolidovan√Ωch center
        avg_consolidated = df_consol['consolidated_centers'].mean()
        
        # Pod√≠l konsolidace (kolik STM ≈°lo do LTM)
        total_consolidated = df_consol['consolidated_centers'].sum()
        
        # Score
        failure_score = 0
        
        # 1. P≈ô√≠li≈° ƒçast√° konsolidace (< 500 krok≈Ø)
        if consolidation_frequency < 500:
            failure_score += 0.3
        
        # 2. P≈ô√≠li≈° m√°lo center konsolidov√°no (< 10)
        if avg_consolidated < 10:
            failure_score += 0.4
        
        # 3. Vysok√° post-fatigue (konsolidace neslevila √∫navu)
        avg_post_fatigue = df_consol['post_fatigue'].mean()
        if avg_post_fatigue > 0.5:
            failure_score += 0.3
        
        failure_score = min(1.0, failure_score)
        
        # Severity
        if failure_score < 0.2:
            severity = "LOW"
            color = "üü¢"
        elif failure_score < 0.5:
            severity = "MEDIUM"
            color = "üü°"
        elif failure_score < 0.8:
            severity = "HIGH"
            color = "üü†"
        else:
            severity = "CRITICAL"
            color = "üî¥"
        
        recommendations = []
        if failure_score > 0.3:
            recommendations.append("Upravit fatigue_threshold (optim√°ln√≠ trigger)")
            recommendations.append("Zv√Ω≈°it consolidation_top_m (v√≠ce center)")
            recommendations.append("Zkontrolovat consolidation_kappa (s√≠lu z√°pisu)")
        
        return RiskAssessment(
            risk_name=f"{color} Consolidation Failure",
            severity=severity,
            score=failure_score,
            description="Riziko neefektivn√≠ konsolidace STM‚ÜíLTM",
            details={
                "n_consolidations": n_consolidations,
                "frequency": f"{consolidation_frequency:.0f} steps",
                "avg_consolidated": f"{avg_consolidated:.1f} centers",
                "total_consolidated": int(total_consolidated),
                "avg_post_fatigue": f"{avg_post_fatigue:.2f}"
            },
            recommendations=recommendations
        )
    
    def analyze_temporal_bias(self) -> RiskAssessment:
        """
        Riziko 5: Temporal Bias
        
        OPRAVENO 2026-01-14:
        - Pou≈æ√≠v√° n_active (poƒçet aktivn√≠ch center) m√≠sto H_mean (ter√©n)
        - Ter√©n m√° DIFUZI, centra maj√≠ LEAK - nelze je zamƒõ≈àovat!
        - Spr√°vn√Ω v√Ωpoƒçet poloƒçasu z decay rate aktivn√≠ch center
        
        Detekce: Syst√©m zapom√≠n√° star√© vzpom√≠nky rychleji/pomaleji ne≈æ m√°.
        """
        if self.metrics_df is None:
            return self._no_data_risk("Temporal Bias")
        
        df = self.metrics_df
        
        # Teoretick√Ω poloƒças LTM: 1 rok = ~18250 interakc√≠
        theoretical_halflife = 18250
        
        # ========================================
        # OPRAVA: Pou≈æ√≠vej n_active m√≠sto H_mean
        # ========================================
        # H_mean je ter√©n s DIFUZ√ç (jin√Ω mechanismus ne≈æ leak)
        # n_active odr√°≈æ√≠ skuteƒçn√Ω decay center
        
        # Najdi sloupec pro interakce (kompatibilita)
        step_col = 'interaction' if 'interaction' in df.columns else 'step'
        steps = df[step_col].values
        n_active = df['n_active'].values
        
        # Pro mƒõ≈ôen√≠ decay pot≈ôebujeme obdob√≠ kde syst√©m u≈æ m√° nƒõjakou historii
        # a NE obdob√≠ r≈Østu (prvn√≠ f√°ze je v≈ædy r≈Øst)
        # Pou≈æijeme posledn√≠ch 30% dat kde je stabiln√≠
        
        start_idx = int(len(n_active) * 0.7)
        if start_idx < 100:
            # P≈ô√≠li≈° m√°lo dat
            return RiskAssessment(
                risk_name="‚ö™ Temporal Bias",
                severity="UNKNOWN",
                score=0.5,
                description="Nedostatek dat pro anal√Ωzu decay",
                details={"reason": "M√©nƒõ ne≈æ 100 datov√Ωch bod≈Ø pro anal√Ωzu"},
                recommendations=["Spustit del≈°√≠ simulaci"]
            )
        
        tail_steps = steps[start_idx:]
        tail_active = n_active[start_idx:]
        
        # Normalizuj na zaƒç√°tek obdob√≠
        initial_active = tail_active[0] if tail_active[0] > 0 else 1
        normalized_active = tail_active / initial_active
        
        # Logaritmick√Ω fit: log(n(t)/n(0)) = -Œªt
        try:
            valid_idx = normalized_active > 0.01  # Ignoruj p≈ô√≠li≈° mal√© hodnoty
            if valid_idx.sum() > 50:
                log_n = np.log(normalized_active[valid_idx])
                relative_steps = tail_steps[valid_idx] - tail_steps[0]
                
                # Linear regression
                coeffs = np.polyfit(relative_steps, log_n, 1)
                estimated_lambda = -coeffs[0]
                
                # Poloƒças z fitted lambda
                if estimated_lambda > 1e-10:
                    estimated_halflife = np.log(2) / estimated_lambda
                elif estimated_lambda < -1e-10:
                    # Z√°porn√° lambda = syst√©m roste (ne decay)
                    estimated_halflife = float('inf')
                    estimated_lambda = 0
                else:
                    estimated_halflife = float('inf')
            else:
                estimated_halflife = float('inf')
                estimated_lambda = 0
        except Exception:
            estimated_halflife = float('inf')
            estimated_lambda = 0
        
        # Score: jak moc se li≈°√≠ od teoretick√©ho
        if estimated_halflife == float('inf'):
            # Syst√©m roste nebo je stabiln√≠ - to m≈Ø≈æe b√Ωt OK pro krat≈°√≠ test
            bias_score = 0.3  # M√≠rn√© riziko
            interpretation = "growing_or_stable"
        else:
            ratio = estimated_halflife / theoretical_halflife
            # Ide√°lnƒõ ratio ‚âà 1.0
            # < 1.0 = p≈ô√≠li≈° rychl√© zapom√≠n√°n√≠
            # > 1.0 = p≈ô√≠li≈° pomal√© zapom√≠n√°n√≠
            if ratio < 0.5:
                bias_score = 0.8  # P≈ô√≠li≈° rychl√©
            elif ratio > 2.0:
                bias_score = 0.6  # P≈ô√≠li≈° pomal√©
            else:
                bias_score = abs(1 - ratio) * 0.5
            interpretation = "measured"
        
        bias_score = min(1.0, bias_score)
        
        # Severity
        if bias_score < 0.2:
            severity = "LOW"
            color = "üü¢"
        elif bias_score < 0.5:
            severity = "MEDIUM"
            color = "üü°"
        elif bias_score < 0.8:
            severity = "HIGH"
            color = "üü†"
        else:
            severity = "CRITICAL"
            color = "üî¥"
        
        recommendations = []
        if estimated_halflife != float('inf') and estimated_halflife < theoretical_halflife * 0.5:
            recommendations.append("Sn√≠≈æit ltm_leak (pomalej≈°√≠ zapom√≠n√°n√≠)")
            recommendations.append("Zkontrolovat, ≈æe homeost√°za se vol√° jednou za interakci")
        elif estimated_halflife != float('inf') and estimated_halflife > theoretical_halflife * 2:
            recommendations.append("Zv√Ω≈°it ltm_leak (rychlej≈°√≠ zapom√≠n√°n√≠)")
        
        return RiskAssessment(
            risk_name=f"{color} Temporal Bias",
            severity=severity,
            score=bias_score,
            description="Riziko nespr√°vn√©ho poloƒçasu decay pamƒõ≈•ov√Ωch center",
            details={
                "theoretical_halflife": f"{theoretical_halflife} interactions",
                "estimated_halflife": f"{estimated_halflife:.0f} interactions" if estimated_halflife != float('inf') else "‚àû (growing/stable)",
                "ratio": f"{estimated_halflife / theoretical_halflife:.2f}" if estimated_halflife != float('inf') else "N/A",
                "lambda_theoretical": "3.8e-5",
                "lambda_estimated": f"{estimated_lambda:.2e}" if estimated_lambda > 0 else "N/A",
                "interpretation": interpretation,
                "data_points_analyzed": int(valid_idx.sum()) if 'valid_idx' in dir() else 0
            },
            recommendations=recommendations
        )
    
    def _no_data_risk(self, risk_name: str) -> RiskAssessment:
        """Placeholder pro chybƒõj√≠c√≠ data."""
        return RiskAssessment(
            risk_name=f"‚ö™ {risk_name}",
            severity="UNKNOWN",
            score=0.0,
            description="Data nejsou k dispozici",
            details={},
            recommendations=[]
        )
    
    def run_all_analyses(self) -> List[RiskAssessment]:
        """Spust√≠ v≈°echny anal√Ωzy rizik."""
        print("\n" + "="*60)
        print("MEMORY RISK ANALYSIS")
        print("="*60 + "\n")
        
        self.risks = [
            self.analyze_catastrophic_interference(),
            self.analyze_capacity_saturation(),
            self.analyze_diffusion_stability(),
            self.analyze_consolidation_failure(),
            self.analyze_temporal_bias()
        ]
        
        return self.risks
    
    def print_report(self):
        """Vytiskne report rizik."""
        if not self.risks:
            self.run_all_analyses()
        
        print("\n" + "="*60)
        print("RISK ASSESSMENT REPORT")
        print("="*60 + "\n")
        
        for risk in self.risks:
            print(f"{risk.risk_name}")
            print(f"  Severity: {risk.severity}")
            print(f"  Score: {risk.score:.2f}")
            print(f"  {risk.description}")
            print(f"\n  Details:")
            for key, value in risk.details.items():
                print(f"    - {key}: {value}")
            
            if risk.recommendations:
                print(f"\n  Recommendations:")
                for rec in risk.recommendations:
                    print(f"    ‚Ä¢ {rec}")
            
            print()
        
        # Overall summary
        avg_score = np.mean([r.score for r in self.risks])
        critical_count = sum(1 for r in self.risks if r.severity == "CRITICAL")
        high_count = sum(1 for r in self.risks if r.severity == "HIGH")
        
        print("="*60)
        print("OVERALL ASSESSMENT")
        print("="*60)
        print(f"Average Risk Score: {avg_score:.2f}")
        print(f"Critical Risks: {critical_count}")
        print(f"High Risks: {high_count}")
        
        if avg_score < 0.3:
            print("‚úÖ Status: HEALTHY - Syst√©m funguje dob≈ôe")
        elif avg_score < 0.6:
            print("‚ö†Ô∏è  Status: MONITORING - Sledujte doporuƒçen√≠")
        else:
            print("üö® Status: ACTION REQUIRED - Okam≈æitƒõ ≈ôe≈°it rizika")
        
        print("="*60 + "\n")
    
    def save_report(self, filename: str = "risk_report.json"):
        """Ulo≈æ√≠ report do JSON."""
        if not self.risks:
            self.run_all_analyses()
        
        report = {
            "timestamp": pd.Timestamp.now().isoformat(),
            "risks": [asdict(r) for r in self.risks],
            "summary": {
                "avg_score": np.mean([r.score for r in self.risks]),
                "critical_count": sum(1 for r in self.risks if r.severity == "CRITICAL"),
                "high_count": sum(1 for r in self.risks if r.severity == "HIGH")
            }
        }
        
        output_path = self.results_dir / filename
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"‚úì Risk report saved to {output_path}")


def main():
    """Entry point."""
    analyzer = MemoryRiskAnalyzer("stress_test_results")
    analyzer.run_all_analyses()
    analyzer.print_report()
    analyzer.save_report()


if __name__ == "__main__":
    main()
